\relax 
\bibstyle{acl_natbib}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Models and Methodology}{1}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Baseline: TF-IDF with SVM}{1}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Recurrent Neural Network: Bi-LSTM}{1}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Transformer-based Model: DistilBERT}{1}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Experiments and Results}{2}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Experimental Setup}{2}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Hyperparameter Tuning for LSTM}{2}{subsection.3.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Performance of LSTM models with different depths and epochs. Best validation accuracy shows the peak performance before significant overfitting.}}{2}{table.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:lstm-tuning}{{1}{2}{Performance of LSTM models with different depths and epochs. Best validation accuracy shows the peak performance before significant overfitting}{table.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Overall Model Comparison}{2}{subsection.3.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Final performance comparison of the three models, focusing on accuracy and training time (CoLab T4 GPU for each models).}}{2}{table.caption.2}\protected@file@percent }
\newlabel{tab:final-comparison}{{2}{2}{Final performance comparison of the three models, focusing on accuracy and training time (CoLab T4 GPU for each models)}{table.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Discussion}{2}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{2}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A}Bi-LSTM Training Visualizations}{3}{appendix.A}\protected@file@percent }
\newlabel{sec:appendix}{{A}{3}{Bi-LSTM Training Visualizations}{appendix.A}{}}
\newlabel{fig:lstm-64-64-4e}{{1a}{3}{64-64 Arch, 4 Epochs}{figure.caption.4}{}}
\newlabel{sub@fig:lstm-64-64-4e}{{a}{3}{64-64 Arch, 4 Epochs}{figure.caption.4}{}}
\newlabel{fig:lstm-64-64-10e}{{1b}{3}{64-64 Arch, 10 Epochs}{figure.caption.4}{}}
\newlabel{sub@fig:lstm-64-64-10e}{{b}{3}{64-64 Arch, 10 Epochs}{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Learning curves for the shallower Bi-LSTM (64-64) architecture.}}{3}{figure.caption.4}\protected@file@percent }
\newlabel{fig:lstm-64-64-curves}{{1}{3}{Learning curves for the shallower Bi-LSTM (64-64) architecture}{figure.caption.4}{}}
\newlabel{fig:lstm-64-128-64-4e}{{2a}{3}{64-128-64 Arch, 4 Epochs}{figure.caption.5}{}}
\newlabel{sub@fig:lstm-64-128-64-4e}{{a}{3}{64-128-64 Arch, 4 Epochs}{figure.caption.5}{}}
\newlabel{fig:lstm-64-128-64-5e}{{2b}{3}{64-128-64 Arch, 5 Epochs}{figure.caption.5}{}}
\newlabel{sub@fig:lstm-64-128-64-5e}{{b}{3}{64-128-64 Arch, 5 Epochs}{figure.caption.5}{}}
\newlabel{fig:lstm-64-128-64-10e}{{2c}{3}{64-128-64 Arch, 10 Epochs}{figure.caption.5}{}}
\newlabel{sub@fig:lstm-64-128-64-10e}{{c}{3}{64-128-64 Arch, 10 Epochs}{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Learning curves for the deeper Bi-LSTM (64-128-64) architecture.}}{3}{figure.caption.5}\protected@file@percent }
\newlabel{fig:lstm-64-128-64-curves}{{2}{3}{Learning curves for the deeper Bi-LSTM (64-128-64) architecture}{figure.caption.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Fine-tuning Progress of DistilBERT}{3}{appendix.B}\protected@file@percent }
\newlabel{sec:appendix}{{B}{3}{Fine-tuning Progress of DistilBERT}{appendix.B}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Training progress of the DistilBERT model, showing loss and validation accuracy at different steps.}}{3}{table.caption.6}\protected@file@percent }
\newlabel{tab:training-progress}{{3}{3}{Training progress of the DistilBERT model, showing loss and validation accuracy at different steps}{table.caption.6}{}}
\gdef \@abspage@last{3}
